{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f44fd9cb-1382-4c8f-a774-a0b85f613daf",
   "metadata": {},
   "source": [
    "### Build concat TS and connectome with chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "06a153c0-e14b-4032-88d6-2ee61c3b05b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loaded 3410 usable chunks from 3410 total chunk IDs.\n",
      "ðŸ“¦ Saved concatenated time series to Results/128/ts_128_with_ids.pkl\n",
      "âœ… Computed connectomes with shape: (3410, 8256)\n",
      "ðŸ“¦ Saved vectorized connectomes to Results/128/connectomes_128.npy\n",
      "ðŸ“¦ Saved corresponding IDs to Results/128/connectome_ids_128.pkl\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from nilearn import connectome\n",
    "from sklearn.covariance import LedoitWolf\n",
    "\n",
    "# === Configuration ===\n",
    "dimension_of_atlas = 128\n",
    "chunked_dir = \"128\"\n",
    "min_timepoints = 10\n",
    "connectome_method = \"tangent\"\n",
    "discard_diagonal = False\n",
    "\n",
    "# === Load chunked data ===\n",
    "all_chunks = []\n",
    "all_chunk_ids = []\n",
    "\n",
    "for filename in os.listdir(chunked_dir):\n",
    "    if filename.endswith(\".pkl\"):\n",
    "        subject_id = filename.replace(\".pkl\", \"\")\n",
    "        with open(os.path.join(chunked_dir, filename), \"rb\") as f:\n",
    "            chunks = pickle.load(f)\n",
    "        for idx, chunk in enumerate(chunks):\n",
    "            if chunk.shape[0] < min_timepoints:\n",
    "                continue\n",
    "            all_chunks.append(chunk)\n",
    "            all_chunk_ids.append(f\"{subject_id}_chunk{idx+1}\")\n",
    "\n",
    "print(f\"âœ… Loaded {len(all_chunks)} usable chunks from {len(all_chunk_ids)} total chunk IDs.\")\n",
    "\n",
    "# === Save all time series (with IDs) ===\n",
    "ts_output_path = f\"Results/128/ts_{dimension_of_atlas}_with_ids.pkl\"\n",
    "with open(ts_output_path, \"wb\") as f:\n",
    "    pickle.dump((all_chunks, all_chunk_ids), f)\n",
    "print(f\"ðŸ“¦ Saved concatenated time series to {ts_output_path}\")\n",
    "\n",
    "# === Compute connectomes ===\n",
    "connectome_measure = connectome.ConnectivityMeasure(\n",
    "    cov_estimator=LedoitWolf(assume_centered=True),\n",
    "    kind=connectome_method,\n",
    "    discard_diagonal=discard_diagonal,\n",
    "    vectorize=True\n",
    ")\n",
    "\n",
    "X = connectome_measure.fit_transform(all_chunks)\n",
    "print(f\"âœ… Computed connectomes with shape: {X.shape}\")\n",
    "\n",
    "# === Save connectomes and IDs ===\n",
    "connectome_path = f\"Results/128/connectomes_{dimension_of_atlas}.npy\"\n",
    "ids_path = f\"Results/128/connectome_ids_{dimension_of_atlas}.pkl\"\n",
    "\n",
    "np.save(connectome_path, X)\n",
    "with open(ids_path, \"wb\") as f:\n",
    "    pickle.dump(all_chunk_ids, f)\n",
    "\n",
    "print(f\"ðŸ“¦ Saved vectorized connectomes to {connectome_path}\")\n",
    "print(f\"ðŸ“¦ Saved corresponding IDs to {ids_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f44fd9cb-1382-4c8f-a774-a0b85f613daf",
   "metadata": {},
   "source": [
    "### Build concat TS and connectome with chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "06a153c0-e14b-4032-88d6-2ee61c3b05b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loaded 140 usable chunks from 140 total chunk IDs.\n",
      "ðŸ“¦ Saved concatenated time series to timeseries/ts_64.pkl\n",
      "âœ… Computed connectomes with shape: (140, 2080)\n",
      "ðŸ“¦ Saved vectorized connectomes to connectome/connectomes_64.npy\n",
      "ðŸ“¦ Saved corresponding IDs to connectome/connectome_ids_64.pkl\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from nilearn import connectome\n",
    "from sklearn.covariance import LedoitWolf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# === Configuration ===\n",
    "dimension_of_atlas = 64\n",
    "chunked_dir = \"timeseries\"\n",
    "min_timepoints = 10\n",
    "connectome_method = \"tangent\"\n",
    "discard_diagonal = False\n",
    "\n",
    "# === Load chunked data ===\n",
    "all_chunks = []\n",
    "all_chunk_ids = []\n",
    "\n",
    "for filename in os.listdir(chunked_dir):\n",
    "    if filename.endswith(\".pkl\"):\n",
    "        subject_id = filename.replace(\".pkl\", \"\")\n",
    "        with open(os.path.join(chunked_dir, filename), \"rb\") as f:\n",
    "            chunks = pickle.load(f)\n",
    "\n",
    "for filename in os.listdir(chunked_dir):\n",
    "    if filename.endswith(\".pkl\"):\n",
    "        subject_id = filename.replace(\".pkl\", \"\")\n",
    "        with open(os.path.join(chunked_dir, filename), \"rb\") as f:\n",
    "            chunks = pickle.load(f)\n",
    "        for idx, chunk in enumerate(chunks):\n",
    "            if chunk.shape[0] < min_timepoints:\n",
    "                continue\n",
    "            all_chunks.append(chunk)\n",
    "            all_chunk_ids.append(f\"{subject_id}_chunk{idx+1}\")\n",
    "\n",
    "print(f\"âœ… Loaded {len(all_chunks)} usable chunks from {len(all_chunk_ids)} total chunk IDs.\")\n",
    "\n",
    "# === Save all time series (with IDs) ===\n",
    "ts_output_path = f\"timeseries/ts_{dimension_of_atlas}.pkl\"\n",
    "with open(ts_output_path, \"wb\") as f:\n",
    "    pickle.dump((all_chunks, all_chunk_ids), f)\n",
    "print(f\"ðŸ“¦ Saved concatenated time series to {ts_output_path}\")\n",
    "\n",
    "# === Compute connectomes ===\n",
    "connectome_measure = connectome.ConnectivityMeasure(\n",
    "    cov_estimator=LedoitWolf(assume_centered=True),\n",
    "    kind=connectome_method,\n",
    "    discard_diagonal=discard_diagonal,\n",
    "    vectorize=True\n",
    ")\n",
    "\n",
    "X = connectome_measure.fit_transform(all_chunks)\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "print(f\"âœ… Computed connectomes with shape: {X.shape}\")\n",
    "\n",
    "# === Save connectomes and IDs ===\n",
    "connectome_path = f\"connectome/connectomes_{dimension_of_atlas}.npy\"\n",
    "ids_path = f\"connectome/connectome_ids_{dimension_of_atlas}.pkl\"\n",
    "\n",
    "np.save(connectome_path, X)\n",
    "with open(ids_path, \"wb\") as f:\n",
    "    pickle.dump(all_chunk_ids, f)\n",
    "\n",
    "print(f\"ðŸ“¦ Saved vectorized connectomes to {connectome_path}\")\n",
    "print(f\"ðŸ“¦ Saved corresponding IDs to {ids_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4bd9b4bb-d7eb-4cee-9627-46bf307d6e5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Loaded 10 chunks\n",
      "\n",
      "ðŸ§© Chunk 1: shape (191, 64)\n",
      "[[ 1.4435895e-01 -6.3068546e-02  1.3269721e-02 ...  5.8984172e-02\n",
      "  -8.6618111e-02 -1.2519098e-02]\n",
      " [-1.2186605e+00 -2.2169331e-01  1.2257959e+00 ... -2.5013125e+00\n",
      "  -5.0428283e-01  6.4696658e-01]\n",
      " [-1.7477064e+00 -1.0675676e+00  4.3724680e-01 ... -2.9192250e+00\n",
      "  -8.4240866e-01  6.9413352e-04]\n",
      " ...\n",
      " [-1.5882313e-01 -2.3469646e-02  9.8152024e-01 ... -2.2017385e-01\n",
      "   1.0668461e+00  5.0219625e-01]\n",
      " [-5.5617493e-01  1.3906076e+00  1.0456649e+00 ...  1.1302136e+00\n",
      "   7.4152881e-01 -5.8765692e-01]\n",
      " [-8.1274837e-01  1.7046607e+00  9.0575117e-01 ...  2.0316153e+00\n",
      "  -4.3403409e-02 -6.1118990e-01]]\n",
      "\n",
      "ðŸ§© Chunk 2: shape (191, 64)\n",
      "[[-0.5154104   0.7496681   0.48824584 ...  1.855288   -0.03866327\n",
      "   0.63850105]\n",
      " [ 0.31425062  0.17197126  0.1687282  ...  0.9502816   0.34955123\n",
      "   1.4889023 ]\n",
      " [ 0.7920628   0.38329828  0.1866765  ...  0.01667785  0.23206249\n",
      "   1.0020478 ]\n",
      " ...\n",
      " [ 0.32825625  0.49986407  0.03864482 ...  1.2517389  -0.18494828\n",
      "   0.18469855]\n",
      " [ 0.36375844 -0.37036192 -0.6418682  ...  0.61508054 -0.1178031\n",
      "   0.2611559 ]\n",
      " [ 0.77592266 -1.3180836  -0.8380801  ...  0.14018281 -0.7563283\n",
      "   0.5268106 ]]\n",
      "\n",
      "ðŸ§© Chunk 3: shape (191, 64)\n",
      "[[ 1.2336874  -1.2559363  -0.5019856  ...  0.5989055  -1.4052646\n",
      "   0.18829161]\n",
      " [ 1.0544878  -0.55662715 -0.06843991 ...  1.2867842  -0.96349937\n",
      "  -1.0739833 ]\n",
      " [ 0.41511086 -0.51216215  0.43479463 ...  1.0929234   0.2124074\n",
      "  -1.994689  ]\n",
      " ...\n",
      " [ 0.41100577 -0.172798    1.9695379  ... -0.37222493  0.42322397\n",
      "   0.32844552]\n",
      " [ 1.4021451   0.10956663  1.0908419  ... -0.1075867   0.26621068\n",
      "  -0.42225692]\n",
      " [ 1.56602     0.45886177  0.3455654  ...  0.1924853   0.01606686\n",
      "  -0.46368688]]\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "# === Path to your .pkl file ===\n",
    "pkl_path = \"timeseries/HCA6058970.pkl\"\n",
    "\n",
    "# === Load the file ===\n",
    "with open(pkl_path, \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "# === Display summary ===\n",
    "print(f\"ðŸ” Loaded {len(data)} chunks\")\n",
    "\n",
    "for idx, chunk in enumerate(data):\n",
    "    chunk = np.asarray(chunk)\n",
    "    print(f\"\\nðŸ§© Chunk {idx+1}: shape {chunk.shape}\")\n",
    "    print(chunk)  # or print(chunk[:5]) to see first 5 rows\n",
    "\n",
    "    if idx == 2:\n",
    "        break  # Show only first 3 chunks to keep output manageable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba0212e-5950-4bcb-998b-022026037109",
   "metadata": {},
   "source": [
    "### DISPLAY DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71af1754-ed93-499d-9ab0-929f9e7aa7f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§  First 10 chunk IDs:\n",
      "['HCA6089375_chunk1', 'HCA6089375_chunk2', 'HCA6089375_chunk3', 'HCA6089375_chunk4', 'HCA6089375_chunk5', 'HCA6089375_chunk6', 'HCA6089375_chunk7', 'HCA6089375_chunk8', 'HCA6089375_chunk9', 'HCA6089375_chunk10']\n",
      "\n",
      "ðŸ•’ First 10 time series shapes:\n",
      "(191, 64)\n",
      "(191, 64)\n",
      "(191, 64)\n",
      "(191, 64)\n",
      "(191, 64)\n",
      "(191, 64)\n",
      "(191, 64)\n",
      "(191, 64)\n",
      "(191, 64)\n",
      "(193, 64)\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Results/64/connectomes_64.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     15\u001b[39m     \u001b[38;5;28mprint\u001b[39m(ts.shape)\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# === Load vectorized connectomes ===\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m connectomes = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mResults/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdimension_of_atlas\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/connectomes_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdimension_of_atlas\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m.npy\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mðŸ§¬ First 10 connectome vectors (shape per vector):\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     20\u001b[39m \u001b[38;5;28mprint\u001b[39m(connectomes[:\u001b[32m10\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/neo_mri/lib/python3.11/site-packages/numpy/lib/_npyio_impl.py:455\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[39m\n\u001b[32m    453\u001b[39m     own_fid = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    454\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m455\u001b[39m     fid = stack.enter_context(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[32m    456\u001b[39m     own_fid = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    458\u001b[39m \u001b[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'Results/64/connectomes_64.npy'"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "dimension_of_atlas = 64  # or 256, 64, etc., depending on your setting\n",
    "\n",
    "# === Load raw time series and IDs ===\n",
    "with open(f\"timeseries/ts_{dimension_of_atlas}.pkl\", \"rb\") as f:\n",
    "    time_series_chunks, chunk_ids = pickle.load(f)\n",
    "\n",
    "print(\"ðŸ§  First 10 chunk IDs:\")\n",
    "print(chunk_ids[:10])\n",
    "\n",
    "print(\"\\nðŸ•’ First 10 time series shapes:\")\n",
    "for ts in time_series_chunks[:10]:\n",
    "    print(ts.shape)\n",
    "\n",
    "# === Load vectorized connectomes ===\n",
    "connectomes = np.load(f\"connectome/{dimension_of_atlas}/connectomes_{dimension_of_atlas}.npy\")\n",
    "print(\"\\nðŸ§¬ First 10 connectome vectors (shape per vector):\")\n",
    "print(connectomes[:10])\n",
    "print(\"Each connectome vector shape:\", connectomes[0].shape)\n",
    "\n",
    "# === Load connectome IDs ===\n",
    "with open(f\"Results/{dimension_of_atlas}/connectome_ids_{dimension_of_atlas}.pkl\", \"rb\") as f:\n",
    "    connectome_ids = pickle.load(f)\n",
    "\n",
    "print(\"\\nðŸ”— First 10 connectome IDs:\")\n",
    "print(connectome_ids[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed04bde3-a597-45d6-9c30-9096aa84b897",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

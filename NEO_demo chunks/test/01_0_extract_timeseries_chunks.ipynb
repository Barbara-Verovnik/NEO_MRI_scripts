{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9897fbdf",
   "metadata": {},
   "source": [
    "### 1 Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9e543c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install nilearn tqdm openpyxl\n",
    "\n",
    "import runpy\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "\n",
    "from nilearn import datasets\n",
    "from nilearn.input_data import NiftiMapsMasker\n",
    "from confounds import extract_confounds\n",
    "\n",
    "from tqdm import tqdm\n",
    "from nilearn.image import load_img\n",
    "from nilearn.maskers import NiftiMapsMasker\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72fdebc3",
   "metadata": {},
   "source": [
    "### 2 Loading the paths to fMRI files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4804d9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load list of preprocessed functional NIfTI files\n",
    "with open(\"rfmri_rest_paths.pkl\", \"rb\") as fp:\n",
    "    func_imgs = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927e8ac2-7ac5-474d-9ea1-fa9f5fab5219",
   "metadata": {},
   "source": [
    "### 3 Loading the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20e93f22-69fd-44dd-8a8a-7c61c4b72185",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "path_to_excel = \"demo_data_bin_test.xlsx\"\n",
    "\n",
    "df = pd.read_excel(path_to_excel)\n",
    "ids = df[\"Subject ID how it's defined in lab/project\"].to_numpy()\n",
    "\n",
    "df_meta = pd.read_excel(path_to_excel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1582f6ee",
   "metadata": {},
   "source": [
    "### 4 Loading the atlas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55bc7123",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">[</span><span style=\"color: #000080; text-decoration-color: #000080\">get_dataset_dir</span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">]</span> Dataset found in <span style=\"color: #800080; text-decoration-color: #800080\">/user/leuven/369/vsc36935/nilearn_data/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">difumo_atlases</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;34m[\u001b[0m\u001b[34mget_dataset_dir\u001b[0m\u001b[1;34m]\u001b[0m Dataset found in \u001b[35m/user/leuven/369/vsc36935/nilearn_data/\u001b[0m\u001b[95mdifumo_atlases\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dimension_of_atlas = 64\n",
    "\n",
    "# Load atlas and gray matter mask\n",
    "fetcher = runpy.run_path('fetcher.py')\n",
    "fetch_difumo = fetcher['fetch_difumo']\n",
    "maps_img = fetch_difumo(dimension=dimension_of_atlas).maps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc47e38",
   "metadata": {},
   "source": [
    "### 4 Fetch grey matter mask from nilearn shipped with ICBM templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3bd78b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">[</span><span style=\"color: #000080; text-decoration-color: #000080\">get_dataset_dir</span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">]</span> Dataset found in <span style=\"color: #800080; text-decoration-color: #800080\">/user/leuven/369/vsc36935/nilearn_data/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">icbm152_2009</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;34m[\u001b[0m\u001b[34mget_dataset_dir\u001b[0m\u001b[1;34m]\u001b[0m Dataset found in \u001b[35m/user/leuven/369/vsc36935/nilearn_data/\u001b[0m\u001b[95micbm152_2009\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#downloads the mask to home/nilearn_data/icbm152_2009\n",
    "gm_mask = datasets.fetch_icbm152_brain_gm_mask(threshold=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b7e270",
   "metadata": {},
   "source": [
    "### 5 Extract timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8643552-42a3-420c-8ce8-9105c4854a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "from nilearn._utils.niimg_conversions import check_same_fov\n",
    "from nilearn.maskers import NiftiMapsMasker  # assuming this import is needed\n",
    "\n",
    "# --- Step 0: Define normalization function ---\n",
    "def normalize_id(id_str):\n",
    "    return id_str.split('_')[0]  # Keep only prefix before first underscore\n",
    "\n",
    "# --- Step 1: Extract subject ID from path ---\n",
    "def extract_subj_id_from_path(path):\n",
    "    # Extract folder name (subject ID with suffixes)\n",
    "    return os.path.basename(os.path.dirname(path))\n",
    "\n",
    "# --- Step 2: Build mapping from normalized subject IDs to functional paths ---\n",
    "func_id_to_path = {}\n",
    "for raw_id, path in func_imgs.items():  # iterating dict keys and values\n",
    "    norm_id = normalize_id(raw_id)\n",
    "    func_id_to_path[norm_id] = path\n",
    "\n",
    "# --- Step 3: Load and normalize metadata IDs ---\n",
    "meta_ids = df_meta[\"Subject ID how it's defined in lab/project\"].astype(str).tolist()\n",
    "meta_ids_norm = [normalize_id(mid) for mid in meta_ids]\n",
    "\n",
    "# --- Step 4: Find common IDs and build aligned lists ---\n",
    "common_ids = [mid for mid in meta_ids_norm if mid in func_id_to_path]\n",
    "print(f\"Subjects with both functional data and metadata (normalized): {len(common_ids)}\")\n",
    "\n",
    "aligned_func_paths = [func_id_to_path[mid] for mid in common_ids]\n",
    "aligned_meta_ids = common_ids\n",
    "\n",
    "assert len(aligned_func_paths) == len(aligned_meta_ids), \"Mismatch after alignment.\"\n",
    "\n",
    "# === Masker parameters ===\n",
    "mask_params = {\n",
    "    'mask_img': gm_mask,\n",
    "    'detrend': True,\n",
    "    'standardize': True,\n",
    "    'high_pass': 0.01,\n",
    "    'low_pass': 0.1,\n",
    "    't_r': 2.53,\n",
    "    'smoothing_fwhm': 6.,\n",
    "    'verbose': 1\n",
    "}\n",
    "masker = NiftiMapsMasker(maps_img=maps_img, **mask_params)\n",
    "\n",
    "# === Output directory ===\n",
    "os.makedirs(\"chunked_data\", exist_ok=True)\n",
    "\n",
    "# === Chunking ===\n",
    "n_chunks = 10\n",
    "num_participants = len(aligned_func_paths)\n",
    "ttotal = datetime.now()\n",
    "\n",
    "for i, (func_img, subj_id) in enumerate(zip(aligned_func_paths, aligned_meta_ids)):\n",
    "    print(f\"\\nProcessing subject {i+1}/{num_participants} ({subj_id})\")\n",
    "    t1 = datetime.now()\n",
    "\n",
    "    confounds = extract_confounds(func_img, mask_img=gm_mask, n_confounds=10)\n",
    "    signals = masker.fit_transform(func_img, confounds=confounds)  # shape: (T, n_ROIs)\n",
    "\n",
    "    T = signals.shape[0]\n",
    "    chunk_len = T // n_chunks\n",
    "    if chunk_len < 10:\n",
    "        print(f\"âš ï¸ Warning: Subject {subj_id} has very short time series ({T} TRs). Skipping.\")\n",
    "        continue\n",
    "\n",
    "    all_chunks = []\n",
    "    for j in range(n_chunks):\n",
    "        start = j * chunk_len\n",
    "        end = (j + 1) * chunk_len if j < n_chunks - 1 else T\n",
    "        chunk_data = signals[start:end, :]\n",
    "        all_chunks.append(chunk_data)\n",
    "\n",
    "    with open(f\"chunked_data/{subj_id}.pkl\", \"wb\") as f:\n",
    "        pickle.dump(all_chunks, f)\n",
    "\n",
    "print(f\"\\nðŸŽ‰ All {num_participants} subjects processed in\", str(datetime.now() - ttotal).split(\".\")[0])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
